<!-- 
https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet
https://www.accordbox.com/blog/web-scraping-framework-review-scrapy-vs-selenium/
-->
# 01.Introduction to Web Scraping
This article is part of a series that goes through all the steps needed to write a script that reads information from a website and save it locally. Make sure that all the [Pre-requisites](00.Pre-requisites.md) are in place before continuing. 

## Installing Selenium and other requirements
Selenium setup requires two steps:
1. Install the Selenium library using the command: __pip install selenium__
1. Download the Selenium WebDriver for your browser (exact version)  
Chrome drivers can be found on [chromium.org](https://chromedriver.chromium.org/downloads)

Scraper setup requires two commands:
1. __pip install requests__ 
2. __pip install beautifulsoup4__

## Scraping a website
### What is web scraping?
Scraping is like browsing to a website and copying some content, but it is done programmatically (e.g. using Python) which means that it is much faster. The limit to how fast you can srape is basically your bandwith and computing power (and also how much the web server alows you to). Technically this process can be divided in two parts:
1. __Crawling__ is the first part, which basically involves opening a page and finding all the interesting links in it, e.g. shops listed in a section of the yellow pages. 
1. __Scraping__ comes next, where all the links from the previous step are visited to extract specific parts of the web page (e.g. the address or phone number).

### Challenges of Scraping
One main challenge is that websites tend to be varied and you will likely end up writing a scraper specific to the site you are dealing with. Even if you are dealing with the same websites, updates/re-designs will likely break your scraper in some way (you will be using the F12 button frequently). 

Some websites do not like to be scraped and will employ different techniques to slow or stop scraping. Another aspect to consider is the legality of this process, which depends on where the server is located, the term of service and what you do once you have the data amongst other things.

An Alternative to web scraping, when available, are Application Programming Interfaces (APIs) since they offer a way to access structured data directly (using formats like JSON and XML, without dealing with the visual presentation of the web pages. So it is always a good idea to check if the website ofers an API before investing time and effort in a scraper.

### Scraping libraries
While there are many ways how to get data from web pages (e.g. using Excel, browser plugins or other tools) this article will focus on how to do it with Python. Having the fleibility of a programming language makes it a very powerful approach and there are very good libraries available such as [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) which will be used in the sample below. There is a very good write up on how to [Build a Web Scraper With Beautiful Soup](https://realpython.com/beautiful-soup-web-scraper-python/). Another framework to consider is [Scrapy](https://scrapy.org/).

### What is Selenium? Why is it needed?
Some websites use JavaScript to load parts of the page later  (not directly when the page is loaded). Some links are also calling JavaScript and the URL to go to is computed on click. These techniques are becoming increasingly common (even as an anti-scraping technique) and unfortunately libraries like Beautiful Soup do not hanlde them well. In comes [Selenium](https://www.selenium.dev/), a framework designed primarily for automated web applications testing. It allows developers to programatically control a browsers using different programming languages. Since with Selenium there is a real browser rendering the page, JavaScript is executed normally and the problems mentioned above can be avoided. This of course requires more reources and makes the whole process slower, so it is wise to use it only when striclty required.

Beautiful Soup and Selenium can also be used together as shown in this [interesting article by freecodecamp.org/](https://www.freecodecamp.org/news/better-web-scraping-in-python-with-selenium-beautiful-soup-and-pandas-d6390592e251/).

### Beautiful Soup scraper
```python
import requests
from bs4 import BeautifulSoup

#Visit Page and parse source HTML
page = requests.get("http://www.python.org")
soup = BeautifulSoup(page.content, 'html.parser')

#Check title is as expected
assert "Python" in soup.title.string

##Perform the search using HTTP GET request
page = requests.get( "https://www.python.org/search/?q=pip&submit=")
soup = BeautifulSoup(page.content, 'html.parser')

#Look for expected result is present
link = soup.find('a', href="/dev/peps/pep-0439/")
assert "PEP 439" in link.string
```
### Scraper using Selenium
```python
from selenium import webdriver

#Open browser and visit page
driver = webdriver.Chrome()
driver.get("http://www.python.org")

#Check title is as expected
assert "Python" in driver.title

#Find search field
search_field = driver.find_element_by_id("id-search-field")
#Enter search term
search_field.send_keys("pip")
#Find and click the Search button
search_button = driver.find_element_by_id("submit")
search_button.click()

#Look for expected result
link = driver.find_element_by_partial_link_text("PEP 439")
#Check the link is correct
assert link.get_attribute("href") == "https://www.python.org/dev/peps/pep-0439/"

#Close browser
driver.close()
```
