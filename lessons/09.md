# Lesson 9: Web scraping

## What is web scraping?

Scraping is like browsing to a website and copying some content, but it is done programmatically (e.g. using Python) which means that it is much faster. The limit to how fast you can scrape is basically your bandwidth and computing power (and how much the web server allows you to). 

Technically this process can be divided in phases:

1.  __Exploring__ is about looking at the age and its code to identify the interesting parts  
1.  __Crawling__ opening a page and finding all the interesting links in it  
e.g. shops listed in a section of the yellow pages. 
1.  __Scraping__ the links from the previous step are visited to extract specific parts of the web page  
e.g. the address or phone number.

### Challenges of Scraping
One main challenge is that websites tend to be varied and you will likely end up writing a scraper specific to every site you are dealing with. Even if you stick with the same websites, updates/re-designs will likely break your scraper in some way (you will be using the F12 button frequently). 

Some websites do not tolerate being scraped and will employ different techniques to slow or stop scraping. Another aspect to consider is the legality of this process, which depends on where the server is located, the term of service and what you do once you have the data amongst other things.

An Alternative to web scraping, when available, are Application Programming Interfaces (APIs) which offer a way to access structured data directly (using formats like JSON and XML) without dealing with the visual presentation of the web pages. Hence it is always a good idea to check if the website offers an API before investing time and effort in a scraper.

### Scraping libraries
While there are many ways how to get data from web pages (e.g. using Excel, browser plugins or other tools) this article will focus on how to do it with Python. Having the flexibility of a programming language makes it a very powerful approach and there are very good libraries available such as [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) which will be used in the sample below. Another framework to consider is [Scrapy](https://scrapy.org/).

### Locating elements 
HTML elements can be located in many ways, for example:
* Eleemnt Type - e.g. A, H1, INPUT or UL
* Element ID - e.g. <p id=element_888>
* Element name - e.g. <input name=buttonOK>
* CSS selector - e.g. <input class=button.sign-up>
* XPath - e.g. //section[@id='main']/div[3]/h3

## Building a scraper
A good starting point is this [Official Beautiful Soup Quick Start](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start).

### Step 0 - Installing the requirements
Scraper setup requires two commands:
1.  pip install requests
1.  pip install beautifulsoup4

### Step 1 - Importing modules
First step is to import the ```requests``` that will download the web pages from the internet and ```BeautifulSoup``` that will parse and use the HTML.

```python
import requests
from bs4 import BeautifulSoup
```

### Step 2 - Downloading python.org search page
The following Python code will download and parse the search results page:

```python
page = requests.get("https://www.python.org/search/?q=strings")
soup = BeautifulSoup(page.content, 'html.parser')
```

### Step 3 - Looking for results
For this lesson we will focus on [python.org](https://python.org), more specifically the search page. For example this URL will search for "strings": https://www.python.org/search/?q=strings. The result HTML page contains this UL:

```html
<ul class="list-recent-events menu">
  <li>
    <h3><a href="/dev/peps/pep-0498/">PEP 498 -- Literal String Interpolation</a></h3>
    <p>... use the same rules as normal...</p>
  </li>
  <li>
    <h3><a href="/dev/peps/pep-0536/">PEP 536 -- Final Grammar for Literal String Interpolation</a></h3>
    <p>...The expression portions of those literals however are subject to certain restrictions...</p>
  </li>
  <li>
    <h3><a href="/dev/peps/pep-0393/">PEP 393 -- Flexible String Representation</a></h3>
    <p>... take up too much memory - especially compared to Python 2.x...</p>
  </li>
</ul>
```

The two most common ways to find items in a page are find() and find_all() for which [official samples](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#searching-the-tree) are available. The snippet below will look for a UL element having CSS class ```list-recent-events```. Then it will look for all the H3 elments which are children of that UL.

```python
ul = soup.find('ul', class_="list-recent-events") 
result_titles = ul.find_all('h3')
```

### Step 4 - Using the results
Finally let's go through all the search results and print the link texts to console.

```python
for result in result_titles:
  print(result.a.text)
```

### Complete scraper using Beautiful Soup

```python
import requests
from bs4 import BeautifulSoup

#Visit Page and parse source HTML
page = requests.get("http://www.python.org")
soup = BeautifulSoup(page.content, 'html.parser')

#Check title is as expected
assert "Python" in soup.title.string

##Perform the search using HTTP GET request
page = requests.get("https://www.python.org/search/?q=pip")
soup = BeautifulSoup(page.content, 'html.parser')

#Look for expected result
link = soup.select_one('a:contains("PEP 439")')
#Get the link URL
print (link['href'])
```
### References
* [Beautiful Soup Official Manual](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
* [Intro to Beautiful Soup](https://programminghistorian.org/en/lessons/intro-to-beautiful-soup)
* [Build a Web Scraper With Beautiful Soup](https://realpython.com/beautiful-soup-web-scraper-python/)
